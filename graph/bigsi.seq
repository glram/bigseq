
import math
#from python import constants
from ..constants import *
#from python import decorators
from ..bloomfilter.bloomfilter import BloomFilter
from ..utils.fncts import convert_query_kmer, non_zero_bitarray_positions, bitwise_and
from ..storage.berkeleydb import BerkeleyDBStorage
from ..scoring.score import Scorer
#from collections import OrderedDict
from ..matrix.bitmatrix import BitMatrix
from index import KmerSignatureIndex
from metadata import SampleMetadata
from bio import seq, kmers
from ..lib.bitarray import bitarray


BLOOMFILTER_SIZE_KEY = "ksi:bloomfilter_size"
DELETION_SPECIAL_SAMPLE_NAME = "D3L3T3D"
PERCENT_KMERS_FOUND_KEY = "percent_kmers_found"
NUM_KMERS_KEY = "num_kmers"
NUM_KMERS_FOUND_KEY = "num_kmers_found"
SAMPLE_KEY = "sample_name"
MIN_UNIQUE_KMERS_IN_QUERY = 0

def validate_build_params(bloomfilters, samples):
    if not len(bloomfilters) == len(samples):
        raise ValueError(
            "There must be the same number of bloomfilters and sample names"
        )

def unpack_and_sum(bitarrays) -> List[int]:
    cumsum = [0 for _ in range(len(bitarrays[0]))]
    for bitarry in bitarrays:
        for i, c in enumerate(bitarry):
            cumsum[i] += int(c)
    return cumsum

def unpack_and_cat(bitarrays) -> List[List[int]]:
    return [[int(v) for v in ba] for ba in bitarrays]

def chunks(l, n):
    n = max(1, n)
    return (l[i : i + n] for i in range(0, len(l), n))

def unpack_and_sum_bitarrays(bitarrays):
    return unpack_and_sum(bitarrays)
    # if j <= 1:
    #     return unpack_and_sum(bitarrays)
    # else:
    #     n = math.ceil(float(len(bitarrays)) / j)
    #     p = Pool(j)
    #     res = p.map(unpack_and_sum, chunks(bitarrays, n))
    #     p.close()
    #     return np.sum(res, axis=0)


def unpack_and_cat_bitarrays(bitarrays):
    return unpack_and_cat(bitarrays)
    # if j <= 1:
    #     return unpack_and_cat(bitarrays)
    # else:
    #     n = math.ceil(float(len(bitarrays)) / j)
    #     p = Pool(j)
    #     res = p.map(unpack_and_cat, chunks(bitarrays, n))
    #     p.close()
    #     return np.vstack(res)

def seq_to_kmers(s: seq):
    ret = Set[K]()
    s |> kmers[K](1) |> convert_query_kmer |> ret.add
    return ret

#import json

class BigsiQueryResult:
    colour: int
    sample_name: str
    num_kmers_found: int
    num_kmers: int
    percent_kmers_found: float
    score: Optional[Dict[str, float]]

    def __init__(self, colour: int, sample_name: str, num_kmers_found: int, num_kmers: int):
        self.colour = colour
        self.sample_name = sample_name
        self.num_kmers_found = num_kmers_found
        self.num_kmers = num_kmers
        self.percent_kmers_found = round(100 * float(num_kmers_found) / num_kmers, 2)
        self.score = None

    def todict(self) -> Dict[str, str]:
        outd = Dict[str, str]()
        if self.score is not None:
            outd = {k: str(self.score[k]) for k in self.score}
        outd[PERCENT_KMERS_FOUND_KEY] = str(self.percent_kmers_found)
        outd[NUM_KMERS_KEY] = str(self.num_kmers)
        outd[NUM_KMERS_FOUND_KEY] = str(self.num_kmers_found)
        outd[SAMPLE_KEY] = str(self.sample_name)
        return outd

    def __eq__(self, ob: BigsiQueryResult):
        return self.todict() == ob.todict()

    def add_score(self, score: Dict[str, float]):
        self.score = score

# todo: figure out seq inheritance?
class BIGSI:
    kmerSignatureIndex: KmerSignatureIndex
    sampleMetadata: SampleMetadata
    storage: BerkeleyDBStorage
    scorer: Scorer

    def __init__(self, storage: BerkeleyDBStorage):
        self.storage = storage
        self.sampleMetadata = SampleMetadata(self.storage)
        self.kmerSignatureIndex = KmerSignatureIndex(self.storage)
        self.scorer = Scorer(self.sampleMetadata.num_samples())

    def bloom(kmers) -> bitarray:
        kmers = [convert_query_kmer(k) for k in kmers]  ## Convert to canonical kmers
        bloomfilter = BloomFilter(BLOOM_SIZE, NUM_HASHES)
        bloomfilter.update(kmers)
        return bloomfilter.bitarr

    def build(storage: BerkeleyDBStorage, bloomfilters, samples) -> BIGSI:
        validate_build_params(bloomfilters, samples)
        SampleMetadata(storage).add_samples(samples)
        ksi = KmerSignatureIndex.create(storage, bloomfilters, BLOOM_SIZE, NUM_HASHES)
        storage.close()  ## Need to delete LOCK files before re init
        return BIGSI(storage)

    def __validate_search_query(self, sequence: seq) -> bool:
        kmer_set = seq_to_kmers(sequence)

        if len(kmer_set) > MIN_UNIQUE_KMERS_IN_QUERY:
            return True
        print(f"Query string should contain at least {MIN_UNIQUE_KMERS_IN_QUERY} unique kmers. Your query contained {len(kmers)} unique kmers, and as a result the false discovery rate may be high. In future this will become an error.")
        return False

    def get_sample_list(self, colours) -> List[int]:
        colours_to_samples = self.sampleMetadata.colours_to_samples(colours)
        return [colours_to_samples[i] for i in colours]

    def exact_filter(self, kmers_to_colours) -> List[BigsiQueryResult]:
        colours_with_all_kmers = [y for y in non_zero_bitarray_positions(
            bitwise_and([x for x in kmers_to_colours.values()])
        )]
        samples = self.get_sample_list(colours_with_all_kmers)
        return [
            BigsiQueryResult(
                colour=c,
                sample_name=s,
                num_kmers=len(kmers_to_colours),
                num_kmers_found=len(kmers_to_colours),
            )
            for c, s in zip(colours_with_all_kmers, samples)
        ]

    def search(self, seq_str: str, threshold=1.0, score=False) -> List[Dict[str, str]]:
        sequence = seq(seq_str)
        self.__validate_search_query(sequence)
        assert threshold <= 1

        kmers = list(seq_to_kmers(sequence))
        kmers_to_colours = self.kmerSignatureIndex.lookup(kmers, remove_trailing_zeros=False)
        min_kmers = math.ceil(len(set(kmers)) * threshold)
        results = self.exact_filter(kmers_to_colours)
        if threshold == 1.0:
            results = self.exact_filter(kmers_to_colours)
        else:
            results = self.inexact_filter(kmers_to_colours, min_kmers)
        if score:
            self.score(kmers, kmers_to_colours, results)
        return [
            r.todict()
            for r in results
            if not r.sample_name == DELETION_SPECIAL_SAMPLE_NAME
        ]

    def inexact_filter(self, kmers_to_colours, min_kmers) -> List[BigsiQueryResult]:
        num_kmers = unpack_and_sum_bitarrays(
            [x for x in kmers_to_colours.values()]
        )
        colours = range(self.sampleMetadata.num_samples())
        colours_to_kmers_found = dict(zip(colours, num_kmers))
        colours_to_kmers_found_above_threshold = self.__colours_above_threshold(
            colours_to_kmers_found, min_kmers
        )
        results = [
            BigsiQueryResult(
                colour=colour,
                sample_name=self.sampleMetadata.colour_to_sample(colour),
                num_kmers_found=int(num_kmers_found),
                num_kmers=len(kmers_to_colours),
            )
            for colour, num_kmers_found in colours_to_kmers_found_above_threshold.items()
        ]
        results.sort(key=lambda x: x.num_kmers_found, reverse=True)
        return results

    def score(self, kmers, kmers_to_colours, results):
        rows = [kmers_to_colours[kmer] for kmer in kmers]
        X = unpack_and_cat_bitarrays(rows)
        for res in results:
            col = "".join([str(int(X[i][res.colour])) for i in range(len(X))])
            ba = bitarray(col)
            score_results = self.scorer.score(ba)
            # score_results["kmer-presence"] = 'hi'
            res.add_score(score_results)

    def __colours_above_threshold(self, colours_to_percent_kmers, min_kmers):
        return {k: v for k, v in colours_to_percent_kmers.items() if v >= min_kmers}

    def insert(self, bloomfilter, sample):
        colour = self.add_sample(sample)
        self.insert_bloom(bloomfilter, colour - 1)

    def delete(self):
        self.storage.delete_all()

    def __validate_merge(self, bigsi):
        assert self.bloomfilter_size == bigsi.bloomfilter_size
        assert self.num_hashes == bigsi.num_hashes
        assert self.kmer_size == bigsi.kmer_size

    def merge(self, bigsi):
        self.__validate_merge(bigsi)
        self.merge_indexes(bigsi)
        self.merge_metadata(bigsi)

storage = BerkeleyDBStorage(DB_FILE_NAME)

bf1 = []
s'ACGTACGTACGTACGTACGTACGTACGTACGACGTACGTACGTACGTGAGTACGTACGTACG' |> kmers[K](1) |> bf1.append
print bf1
bloomfilter1 = BIGSI.bloom(bf1)
bloomfilter2 = BIGSI.bloom([k'ACGTACGTACGTACGTGAGTACGTACGTACG'])
big = BIGSI.build(storage, [bloomfilter1, bloomfilter2], ['a', 'b'])
print big.search('ACGTACGTACGTACGTACGTACGTACGTACG')
print big.search('ACGTACGTACGTACGTGAGTACGTACGTACG')
print big.search('TCGTACGTACGTACGTGAGTACGTACGTACG')
